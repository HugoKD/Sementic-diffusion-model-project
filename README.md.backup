# Segmentation Guided Diffusion Model for lung radio generation

This repository contains the implementation of a Segmentation Guided Diffusion model designed to generate high-quality lung images from segmentation masks. The model specifically focuses on generating lung images with regions affected by specific diseases, providing a valuable tool for medical imaging and research.

![Generated lung image](ddpm-lungs-512-segguided/samples_many_100/condon_ts_133.png)

## Why use our model?

The method used in this project outperforms existing segmentation-guided image generative models (like [SPADE](https://github.com/NVlabs/SPADE), [ControlNet](https://github.com/lllyasviel/ControlNet) or [Pix2Pix](https://github.com/phillipi/pix2pix) in terms of the faithfulness of generated images to input masks, on lung radio medical image datasets, and is on par for anatomical realism. It is also simple to use and train, and its precise pixel-wise obedience to input segmentation masks is due to it always operating in the native image space (it's not a latent diffusion model), which is especially helpful when conditioning on the complex and detailed anatomical structure of the lung.


**Using this code, you can:**
1. Train a segmentation-guided (or standard unconditional) diffusion model on your own lung radios dataset, with a wide range of options.
2. Generate lung radios from these models

## Getting Started

### Prerequisites

This codebase was created with Python 3.11. First, install PyTorch for your computer's CUDA version (check it by running `nvidia-smi` if you're not sure) according to the provided command at https://pytorch.org/get-started/locally/; 

Next adapt and run `pip3 install -r requirements.txt` to install the required packages.

### Deployment

We provide pre-trained model checkpoints (`.safetensor` files) and config (`.json`) on [Google Drive](https://drive.google.com/drive/folders/1VX_PzfoF_5O4xBrQGVsZhDfh8mK6Omzo?usp=sharing).

Once you've downloaded the checkpoint and config file for your model of choice, please:

Put both files in a directory called `ddpm-lungs-512-segguided/unet` to use it with our evaluation code. 

Next, you can proceed to the [**Evaluation/Sampling**]() section below to generate images from these models.

## Train your own model

### Data Preparation

Please put your training images in some dataset directory `DATA_FOLDER`, organized into train, validation and test split subdirectories. The images should be in a format that PIL can read (e.g. `.png`, `.jpg`, etc.). For example:

``` 
DATA_FOLDER
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ tr_1.png
â”‚   â”œâ”€â”€ tr_2.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val
â”‚   â”œâ”€â”€ val_1.png
â”‚   â”œâ”€â”€ val_2.png
â”‚   â””â”€â”€ ...
â””â”€â”€ test
    â”œâ”€â”€ ts_1.png
    â”œâ”€â”€ ts_2.png
    â””â”€â”€ ...
```

If you have segmentation masks, put them in a similar directory structure in a separate folder `MASK_FOLDER`, with a subdirectory `all` that contains the split subfolders, as shown below. **Each segmentation mask should have the same filename as its corresponding image in `DATA_FOLDER`, and should be saved with integer values starting at zero for each object class, i.e., 0, 1, 2,...**.

If you don't want to train a segmentation-guided model, you can skip this step.

``` 
MASK_FOLDER
â”œâ”€â”€ all
â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”œâ”€â”€ tr_1.png
â”‚   â”‚   â”œâ”€â”€ tr_2.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val
â”‚   â”‚   â”œâ”€â”€ val_1.png
â”‚   â”‚   â”œâ”€â”€ val_2.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ test
â”‚       â”œâ”€â”€ ts_1.png
â”‚       â”œâ”€â”€ ts_2.png
â”‚       â””â”€â”€ ...
```

### Training

The basic command for training a standard unconditional diffusion model is

```bash
CUDA_VISIBLE_DEVICES={DEVICES} python3 main.py \
    --mode train \
    --model_type DDIM \
    --img_size {IMAGE_SIZE} \
    --num_img_channels {NUM_IMAGE_CHANNELS} \
    --dataset {DATASET_NAME} \
    --img_dir {DATA_FOLDER} \
    --train_batch_size 16 \
    --eval_batch_size 8 \
    --num_epochs 400
```

where:
- `DEVICES` is a comma-separated list of GPU device indices to use (e.g. `0,1,2,3`).
- `IMAGE_SIZE` and `NUM_IMAGE_CHANNELS` respectively specify the size of the images to train on (e.g. `256`) and the number of channels (1 for greyscale, 3 for RGB).
- `model_type` specifies the type of diffusion model sampling algorithm to evaluate the model with, and can be `DDIM` or `DDPM`.
- `DATASET_NAME` is some name for your dataset 
- `DATA_FOLDER` is the path to your dataset directory, as outlined in the previous section.
- `--train_batch_size` and `--eval_batch_size` specify the batch sizes for training and evaluation, respectively. We use a train batch size of 16 for one 48 GB A6000 GPU for an image size of 256.
- `--num_epochs` specifies the number of epochs to train for (our default is 400).

To preprocess our dataset, we have written a python script: [preprocessing.py](preprocessing.py).

### Adding segmentation guidance, mask-ablated training, and other options

To train your model with mask guidance, simply add the options:
```bash
    --seg_dir {MASK_FOLDER} \
    --segmentation_guided \
    --num_segmentation_classes {N_SEGMENTATION_CLASSES} \
```

where:
- `MASK_FOLDER` is the path to your segmentation mask directory, as outlined in the previous section.
- `N_SEGMENTATION_CLASSES` is the number of classes in your segmentation masks, **including the background (0) class**.

To also train your model with mask ablation (randomly removing classes from the masks to each the model to condition on masks with missing classes; see our paper for details), simply also add the option `--use_ablated_segmentations`.

The full command is written in [execute.sh](execute.sh).

## Evaluation/Sampling

Sampling images with a trained model is run similarly to training. For example, 100 samples from an unconditional model can be generated with the command:

```bash
CUDA_VISIBLE_DEVICES={DEVICES} python3 main.py \
    --mode eval_many \
    --model_type DDIM \
    --img_size 256 \
    --num_img_channels {NUM_IMAGE_CHANNELS} \
    --dataset {DATASET_NAME} \
    --eval_batch_size 8 \
    --eval_sample_size 100
```

Note that the code will automatically use the checkpoint from the training run, and will save the generated images to a directory called `samples` in the model's output directory. To sample from a model with segmentation guidance, simply add the options:

```bash
    --seg_dir {MASK_FOLDER} \
    --segmentation_guided \
    --num_segmentation_classes {N_SEGMENTATION_CLASSES} \
```

This will generate images conditioned on the segmentation masks in `MASK_FOLDER/all/test`. Segmentation masks should be saved as image files (e.g., `.png`) with integer values starting at zero for each object class, i.e., 0, 1, 2.

The full command is written in [evaluate.sh](evaluate.sh).

## Additional Options/Config
Our code has further options for training and evaluation; run `python3 main.py --help` for more information. Further settings still can be changed under `class TrainingConfig:` in `training.py` (some of which are exposed as command-line options for `main.py`, and some of which are not).

## Built With

Thank you to Hugging Face's awesome  library for providing a helpful backbone for our code!

* [**Diffusers**](https://github.com/huggingface/diffusers) - Hugging Face library

Thanks to Nicholas Konz, Yuwen Chen, Haoyu Dong, and Maciej A. Mazurowsk for their work on the method used in this project.

```bib
@misc{konz2024anatomicallycontrollable,
      title={Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models}, 
      author={Nicholas Konz and Yuwen Chen and Haoyu Dong and Maciej A. Mazurowski},
      year={2024},
      eprint={2402.05210},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}
```

## Contributing

Please leave a star ðŸŒŸ and read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to improve the project.

## Authors

* **Luka Lafaye de Micheaux** - *Initial work* - [lukalafaye](https://github.com/lukalafaye)
* **Hugo Cadet** - *Initial work* - [HugoKD](https://github.com/HugoKD)

See also the list of [contributors](https://github.com/lukalafaye/SDM/contributors) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details

---

<p align="center">Copyright Â© 2024 Luka Lafaye de Micheaux | All Rights Reserved.</p>
